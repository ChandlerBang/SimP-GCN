# SimP-GCN
A Pytorch implementation of "Node Similarity Preserving Graph Convolutional Networks" (WSDM 2021). [[paper]](https://arxiv.org/abs/2011.09643)

<div align=center><img src="https://github.com/ChandlerBang/SimP-GCN/blob/main/SimP-GCN.png" width="700"/></div>

## Abstract 
Graph Neural Networks (GNNs) explore the graph structure and node features by aggregating and transforming information within node neighborhoods. However, through theoretical and empirical analysis, we reveal that the aggregation process of GNNs tends to destroy node similarity in the original feature space. But there are many scenarios where node similarity plays a crucial role. Thus, it has motivated the proposed framework SimP-GCN that can effectively and efficiently preserve node similarity while exploiting graph structure. We validate the effectiveness of SimP-GCN on seven benchmark datasets including three assortative and four disassorative graphs. The results demonstrate that SimP-GCN outperforms representative baselines. Further probe shows various advantages of the proposed framework.


## Requirements
See that in https://github.com/ChandlerBang/SimP-GCN/blob/main/requirements.txt
```
tqdm==4.42.0
torch==1.2.0
ipdb==0.13.2
scipy==1.4.1
dgl==0.5.2
numpy==1.17.4
pandas==1.0.3
networkx==2.3
scikit_learn==0.23.2
tensorboardX==2.1
```

## Installation
```
pip install -r requirements.txt
```

## Reproduce the results
All the hyper-parameters settings are included in [`scripts`](https://github.com/ChandlerBang/SimP-GCN/tree/master/scripts) folder. For example, you can run the following commands.
```
sh scripts/assortative/cora.sh
sh scripts/disassortative/actor.sh
```

You can run `python run.py` to get all the results. Make sure you have installed the same version of depedencies.  

Note that for adversarial robustness, you need to install [DeepRobust](https://github.com/DSE-MSU/DeepRobust) as follows:
```
git clone https://github.com/DSE-MSU/DeepRobust.git
cd DeepRobust
python setup.py install
```
You might also need to delete the saved cora temporary files generated by experiments on assortative graphs. It is because when evaluating adversarail robustness we use the largest connected component of the graph instead of the entire graph.
```
rm saved/cora* 
sh scripts/adversarial/cora.sh
```
You can also choose to run the test example ([test_simpgcn.py](https://github.com/DSE-MSU/DeepRobust/blob/master/examples/graph/test_simpgcn.py)) in DeepRobust. 
```
python DeepRobust/deeprobust/exmaples/graphs/test_simpgcn.py --dataset cora --ptb_rate 0.2
```


## Acknowledgement
This repository is modified from [SelfTask-GNN](https://github.com/ChandlerBang/SelfTask-GNN), [DropEdge](https://github.com/DropEdge/DropEdge) and [Geom-GCN](https://github.com/graphdml-uiuc-jlu/geom-gcn). We sincerely thank them for their contributions.

## Cite
For more information, you can take a look at the [paper](https://arxiv.org/abs/2011.09643).

If you find this repo to be useful, please cite our paper. Thank you.
```
@misc{jin2020node,
      title={Node Similarity Preserving Graph Convolutional Networks}, 
      author={Wei Jin and Tyler Derr and Yiqi Wang and Yao Ma and Zitao Liu and Jiliang Tang},
      year={2020},
      eprint={2011.09643},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

<!--
See that in https://github.com/DSE-MSU/DeepRobust/blob/master/requirements.txt
```
matplotlib==3.1.1
numpy==1.17.1
torch==1.2.0
scipy==1.3.1
torchvision==0.4.0
texttable==1.6.2
networkx==2.4
numba==0.48.0
Pillow==7.0.0
scikit_learn==0.22.1
skimage==0.0
tensorboardX==2.0
```

## Installation
To run the code, first you need to clone DeepRobust
```
git clone https://github.com/DSE-MSU/DeepRobust.git
cd DeepRobust
python setup.py install
```

## Run the code
After installation, you can clone this repository
```
git clone https://github.com/ChandlerBang/Pro-GNN.git
cd Pro-GNN
python train.py --dataset polblogs --attack meta --ptb_rate 0.15 --epoch 1000
```

## Reproduce the results
All the hyper-parameters settings are included in [`scripts`](https://github.com/ChandlerBang/Pro-GNN/tree/master/scripts) folder. Note that same hyper-parameters are used under different perturbation for the same dataset. 

To reproduce the performance reported in the paper, you can run the bash files in folder `scripts`.
```
sh scripts/meta/cora_meta.sh
```
**IMPORTANT NOTICE** For the performance of Pubmed dataset, if you don't add the code (line 59-62 in `train.py`), the performance of GCN should be around 85 since the data splits are different from what I used in the paper. See details in https://github.com/ChandlerBang/Pro-GNN/issues/2


## Generate attack by yourself
With the help of DeepRobust, you can run the following code to generate meta attack
```
python generate_attack.py --dataset cora --ptb_rate 0.05 --seed 15
```

## Cite
For more information, you can take a look at the [paper](https://arxiv.org/abs/2005.10203) or the detailed [code](https://github.com/DSE-MSU/DeepRobust/blob/master/deeprobust/graph/defense/prognn.py) shown in DeepRobust.

If you find this repo to be useful, please cite our paper. Thank you.
```
@article{jin2020node,
  title={Graph Structure Learning for Robust Graph Neural Networks},
  author={Jin, Wei and Ma, Yao and Liu, Xiaorui and Tang, Xianfeng and Wang, Suhang and Tang, Jiliang},
  journal={arXiv preprint arXiv:2005.10203},
  year={2020}
}
```
-->

